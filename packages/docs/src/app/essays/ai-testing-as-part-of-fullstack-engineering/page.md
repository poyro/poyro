---
title: AI Testing as part of Full-Stack Engineering
---

AI used to be the stuff of PhDs, research scientists, and machine learning engineers. As a full-stack engineer, if you collaborated with one of these folks you might have received an API endpoint from them. You could send some data to it, some magic would happen behind the scenes, and voil√† maybe you got a label for an image or a sentiment score for a customer review.

The world was changed by ChatGPT. There is still a magical API endpoint. However, some of that work that was done by machine learning engineers is now falling into the hands of full-stack engineers.

## Evaluation = Unit Testing

One of the critical tasks that ML engineers used to do was "evaluation". In a nutshell, you'd set some examples to the side of what data you wanted your AI to output. You would compare your AI's actual output to the desired output. The ML engineer would use this signal to iterate on the model, using sophisticated optimization algorithms.

Does this sound familiar? It should, because:

- The evaluation step here is almost exactly the same as unit testing. The difference: in typical unit testing we want all tests to pass, but with AI systems we are ok with a high percentage of our tests passing as long as our changes **don't decrease** that percentage.
- The iteration step the ML engineer used to do has been replaced by prompt engineering, retrieval-augmented generation (RAG), system design (e.g. chaining), and in rare cases fine-tuning.

The process of evaluating AI apps is the same as regular development: Instead of writing just code, now you write code to retrieve data, chain prompts, and the prompts themselves. Just like before, we need to write tests to make sure our code does what we want. This time however, we need to write tests not just with code but also with prompts.

## LLM-Based Unit Testing

There are many ways in which we can use regular code to write unit tests for AI outputs. For example, if you ask ChatGPT to generate a JSON response for your application you can check it obeys the schema you desire. If your Claude call is generating code you can run a syntax checker. You may have an application where you use a multi-modal AI to label an image of a restaurant's item as food or a drink. You could set up a handful of food and drink images and check if the enum generated by the AI matches the content of the image.

But what if the output is not "structured" in one of these ways? What if we are generating an open ended response to a customer support query? What if we are generating text for a new clause to add to a contract? 

And what if the condition that we want to check is open ended? Instead of checking for "food" vs. "drink", what if we want to check that our SEO-generation AI app "contains a call to action"? A call to action could look a lot of different ways.

These open ended questions and unstructured inputs are exactly what LLMs are good at dealing with. If Claude with one prompt generated our SEO text, Claude with another prompt can check if it has a call to action. It doesn't have to be the same model doing the task and doing the check though.

## Bridging Testing for AI and Full-Stack Engineering

There are many great open source tools out there to do LLM-based unit testing. However, some of the most popular tools are configuration driven or introduce new APIs, which makes it difficult to write your LLM-based unit tests alongside your regular unit tests. Many of the most popular tools are Python first, despite [JavaScript being ranked as the most popular language by Stack Overflow 11 years in a row](https://survey.stackoverflow.co/2023/#most-popular-technologies-language-prof) and being popular amongst full-stack engineers.

There are also good SaaS solutions. However, these often have unnecessary UI components (e.g. you have to define your tests in the UI) which are awkward for the typical software engineering workflow. Many of these solutions only support LLM-based evaluation provided through a model vendor like ChatGPT or Claude. This means you can only unit test your app as much as your budget allows, an odd concept for an engineer.

This is exactly the set of problems that [Poyro](/) set out to solve:

- Poyro is written in JavaScript to be easily usable by node developers.
- Poyro doesn't introduce new APIs or cofigurations. You can write LLM-based unit tests in the syntax of Jest right alongside all your other tests. 
- Poyro does not include any unnecessary UI components. Your tests are written as code.
- Poyro tests using a small LLM that [runs locally on your computer](/how-does-it-work), so no fees to model provides.

In addition, we kept the [API simple](/sdk-reference), introducing only a single matcher rather than creating a bunch of new abstractions you have to learn.

---

Do you agree that AI is becoming part of full-stack and web engineering? Is AI the best to test your AI app? Should AI testing tools be tightly integrated with your stack or is this not super important? Tell us what you think in our [Discord](https://discord.com/invite/gmCjjJ5jSf)!